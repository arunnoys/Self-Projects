# -*- coding: utf-8 -*-
"""Data Extraction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sEaTAttimtT7iRIpmAfYot6SVTc4oSzI

# **Data Extraction:**
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import os

def extract_article(url, filename):
    try:
        page = requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')
        title = soup.find('h1').text.strip()

        texts = soup.findAll(attrs={'class': 'td-post-content tagdiv-type'})
        if texts:
            body = texts[0].text.replace('\n'," ")
            filepath = os.path.join('extracted_texts', filename + '.txt')
            with open(filepath, 'w', encoding='utf-8') as file:
                file.write(f"{title}\n\n")
                file.write(body)
            print(f"Article extracted and saved successfully: {filename}")

        else:
            texts2 = soup.findAll(attrs={'class': 'td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type'})
            body = texts2[0].text.replace('\n'," ")
            filepath = os.path.join('extracted_texts', filename + '.txt')
            with open(filepath, 'w', encoding='utf-8') as file:
                file.write(f"{title}\n\n")
                file.write(body)
            print(f"Article extracted and saved successfully: {filename}")
        return title, body

    except Exception as e:
        print(f"Error extracting article from {url}: {e}")
        return None, None

extracted_data = []

if not os.path.exists('extracted_texts'):
    os.makedirs('extracted_texts')

df = pd.read_excel('Input.xlsx')

for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    filename = f"{url_id}"
    title, body = extract_article(url, filename)
    if title and body:
        extracted_data.append({'title': title, 'body': body})

df_extracted = pd.DataFrame(extracted_data)
csv_filename = 'all_extracted_texts.csv'
df_extracted.to_csv(csv_filename, index=False)
print(f"All extracted texts saved to: {csv_filename}")

"""Error in "blackassign0036" & "blackassign0049" because the URL provided does not exist or the server could not locate the webpage associated with it."""

# import shutil
# directory_path = '/content/extracted_texts'
# shutil.rmtree(directory_path)
# print(f"Directory '{directory_path}' deleted successfully.")

# import shutil
# shutil.make_archive('extracted_texts', 'zip', 'extracted_texts')

"""# **Preprocessing:**"""

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
# nltk.download('all')

df_new=pd.read_csv('/content/all_extracted_texts.csv')

df_new

import pandas as pd

with open('/content/StopWords_Auditor.txt', 'r') as file:
    exclude_words1 = [line.split()[0] for line in file]
with open('/content/StopWords_Currencies.txt', 'r') as file:
    exclude_words2 = [line.split()[0] for line in file]
with open('/content/StopWords_DatesandNumbers.txt', 'r') as file:
    exclude_words3 = [line.split()[0] for line in file]
with open('/content/StopWords_Generic.txt', 'r') as file:
    exclude_words4 = [line.split()[0] for line in file]
with open('/content/StopWords_GenericLong.txt', 'r') as file:
    exclude_words5 = [line.split()[0] for line in file]
with open('/content/StopWords_Geographic.txt', 'r') as file:
    exclude_words6 = [line.split()[0] for line in file]
with open('/content/StopWords_Names.txt', 'r') as file:
    exclude_words7 = [line.split()[0] for line in file]

def clean_text1(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words1]
    return ' '.join(cleaned_words)
def clean_text2(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words2]
    return ' '.join(cleaned_words)
def clean_text3(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words3]
    return ' '.join(cleaned_words)
def clean_text4(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words4]
    return ' '.join(cleaned_words)
def clean_text5(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words5]
    return ' '.join(cleaned_words)
def clean_text6(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words6]
    return ' '.join(cleaned_words)
def clean_text7(text):
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in exclude_words7]
    return ' '.join(cleaned_words)

df_new['body'] = df_new['body'].apply(clean_text1)
df_new['body'] = df_new['body'].apply(clean_text2)
df_new['body'] = df_new['body'].apply(clean_text3)
df_new['body'] = df_new['body'].apply(clean_text4)
df_new['body'] = df_new['body'].apply(clean_text5)
df_new['body'] = df_new['body'].apply(clean_text6)
df_new['body'] = df_new['body'].apply(clean_text7)

cleaned_csv_filename = 'cleaned_extracted_texts.csv'
df_new.to_csv(cleaned_csv_filename, index=False)
print(f"Cleaned text saved to: {cleaned_csv_filename}")

"""# **Positive Score:**"""

import pandas as pd

with open('/content/positive-words.txt', 'r', encoding='utf-8') as file:
    positive_words = [line.split()[0] for line in file]

df_processed = pd.read_csv('/content/cleaned_extracted_texts.csv')

def count_positive_words(text):
    count = 0
    for word in positive_words:
        if word.lower() in text.lower():
            count += 1
    return count

positive_scores = []

for index, row in df_processed.iterrows():
    positive_score = count_positive_words(row['body'])
    positive_scores.append(positive_score)

print("Calculated positive scores:", positive_scores)

df_output= pd.read_excel('/content/Output Data Structure.xlsx')

rows_to_remove = [35,48]

df_output=df_output.drop(rows_to_remove)

df_output= df_output.reset_index(drop=True)

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Rows removed and file saved successfully.")

df_output['POSITIVE SCORE'] = positive_scores

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Negative Score:**"""

import pandas as pd

with open('/content/negative-words.txt', 'r', encoding='utf-8') as file:
    negative_words = [line.split()[0] for line in file]

df_processed = pd.read_csv('/content/cleaned_extracted_texts.csv')

def count_negative_words(text):
    count = 0
    for word in negative_words:
        if word.lower() in text.lower():
            count += 1
    return count

negative_scores = []

for index, row in df_processed.iterrows():
    negative_score = count_negative_words(row['body'])
    negative_scores.append(negative_score)

print("Calculated negative scores:", negative_scores)

df_output['NEGATIVE SCORE'] = negative_scores

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Polarity Score:**"""

polarity_scores = []

for positive_score, negative_score in zip(positive_scores, negative_scores):
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    polarity_scores.append(polarity_score)

# polarity_scores = np.array(polarity_scores)

print("Polarity scores:", polarity_scores)

df_output['POLARITY SCORE'] = polarity_scores

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Subjective Score:**"""

num_words_array = []

for text in df_processed['body']:
    words = text.split()
    num_words = len(words)
    num_words_array.append(num_words)

# num_words_array = np.array(num_words_array)

print("Number of words in each row:", num_words_array)

subjectivity_scores = []

for positive, negative, num_words in zip(positive_scores, negative_scores, num_words_array):
    subjectivity_score = (positive + negative) / (num_words + 0.000001)
    subjectivity_scores.append(subjectivity_score)

print("Subjectivity Scores:", subjectivity_scores)

df_output['SUBJECTIVITY SCORE'] = subjectivity_scores

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Average Sentence Length:**"""

import re

num_sentences_array = []

for text in df_processed['body']:
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
    num_sentences = len(sentences)  # Calculate the number of sentences
    num_sentences_array.append(num_sentences)  # Append the number of sentences to the array

# num_sentences_array = np.array(num_sentences_array)

print("Number of sentences in each row:", num_sentences_array)

average_sentence_len = []

for average, num_words in zip(num_sentences_array, num_words_array):
    average_length = (num_words)/(average)
    average_sentence_len.append(average_length)

print("Average Sentence Length:", average_sentence_len)

df_output['AVG SENTENCE LENGTH'] = average_sentence_len

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Percentage of Complex Words:**"""

nltk.download('all')

from nltk.corpus import cmudict

cmu_dict = cmudict.dict()

def count_syllables(word):
    if word.lower() in cmu_dict:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]])
    else:
        return 0

num_complex_words_array = []

for text in df_processed['body']:
    words = text.split()
    num_complex_words = sum(1 for word in words if count_syllables(word) > 2)
    num_complex_words_array.append(num_complex_words)

# num_complex_words_array = np.array(num_complex_words_array)

print("Number of complex words in each row:", num_complex_words_array)

complex_percentage = []

for comp, num_words in zip(num_complex_words_array, num_words_array):
    percen = (comp)/(num_words)
    complex_percentage.append(percen)

print("Percentage of Complex Words:", complex_percentage)

df_output['PERCENTAGE OF COMPLEX WORDS'] = complex_percentage

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Fog Index:**"""

fog = []

for comp, sentence in zip(complex_percentage, average_sentence_len):
    fg = 0.4*(comp+sentence)
    fog.append(fg)

print("Fog Index:", fog)

df_output['FOG INDEX'] = fog

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Average Number of Words Per Sentence:**"""

df_output['AVG NUMBER OF WORDS PER SENTENCE'] = average_sentence_len

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Complex Word Count:**"""

df_output['COMPLEX WORD COUNT'] = num_complex_words_array

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Word Count:**"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
import string

# nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

df_word = pd.read_csv('/content/all_extracted_texts.csv')

def clean_and_count_words(text):
    words = nltk.word_tokenize(text)
    cleaned_words = []
    for word in words:
        word = word.translate(str.maketrans('', '', string.punctuation))
        if word.lower() not in stop_words and word != '':
            cleaned_words.append(word.lower())
    return len(cleaned_words)

total_cleaned_words = []
for text in df_word['body']:
    total_cleaned_words.append(clean_and_count_words(text))

# total_cleaned_words = np.array(total_cleaned_words)

print("Total cleaned words in each row:", total_cleaned_words)

df_output['WORD COUNT'] = total_cleaned_words

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Syllable Count Per Word:**"""

import pandas as pd
import re

def count_syllables(word):
    vowels_pattern = re.compile(r'[aeiouyAEIOUY]+')
    exceptions = ['es', 'ed']
    syllable_count = 0
    exception_flag = False
    for i, char in enumerate(word):
        if char.lower() in 'aeiouy' and (word[i:i+2].lower() not in exceptions):
            syllable_count += 1
            if word[i:i+2].lower() in exceptions:
                exception_flag = True
        elif char.lower() not in 'aeiouy':
            exception_flag = False
    if word.endswith('e') and not exception_flag:
        syllable_count -= 1
    syllable_count = max(1, syllable_count)
    return syllable_count

num_syllables_array = []

for text in df_processed['body']:
    words = text.split()
    syllables_per_row = sum(count_syllables(word) for word in words)
    num_syllables_array.append(syllables_per_row)

# num_syllables_array = np.array(num_syllables_array)

print("Number of syllables in each row:", num_syllables_array)

df_output['SYLLABLE PER WORD'] = num_syllables_array

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Personal Pronouns:**"""

import pandas as pd
import re

personal_pronouns_pattern = re.compile(r'\b(?:I|we|my|ours|us)\b', flags=re.IGNORECASE)

personal_pronouns_counts = []

for text in df_processed['body']:
    matches = re.findall(personal_pronouns_pattern, text)
    matches = [match for match in matches if match.lower() != 'us']
    pronoun_count = len(matches)
    personal_pronouns_counts.append(pronoun_count)

# personal_pronouns_counts = np.array(personal_pronouns_counts)

print("Personal pronouns counts in each row:", personal_pronouns_counts)

df_output['PERSONAL PRONOUNS'] = personal_pronouns_counts

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output

"""# **Average Word Length:**"""

num_characters_array = []

for text in df_processed['body']:
    num_characters = len(text)
    num_characters_array.append(num_characters)

# num_characters_array = np.array(num_characters_array)

print("Number of characters in each row:", num_characters_array)

avg_word = []

for char, num_words in zip(num_characters_array, num_words_array):
    avg = (char)/(num_words)
    avg_word.append(avg)

print("Average Word Length::", avg_word)

df_output['AVG WORD LENGTH'] = avg_word

df_output.to_excel('/content/Output Data Structure.xlsx', index=False)
print("Column replaced and file saved successfully.")

df_output